{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Encoders\n",
    "\n",
    "import java.io.File\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import java.lang.Math._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@670a2f78\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@670a2f78"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkPreProcessing\").getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getListOfFiles: (dir: String)List[java.io.File]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// http://alvinalexander.com/scala/how-to-list-files-in-directory-filter-names-scala/\n",
    "\n",
    "def getListOfFiles(dir: String):List[File] = {\n",
    "    val d = new File(dir)\n",
    "    if (d.exists && d.isDirectory) {\n",
    "        d.listFiles.filter(_.isFile).toList\n",
    "    } else {\n",
    "        List[File]()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder = ./2019/\n",
       "csvFiles = List(./2019/201902-citibike-tripdata.csv, ./2019/201911-citibike-tripdata.csv, ./2019/201903-citibike-tripdata.csv, ./2019/201908-citibike-tripdata.csv, ./2019/201912-citibike-tripdata.csv, ./2019/201906-citibike-tripdata.csv, ./2019/201901-citibike-tripdata.csv, ./2019/201909-citibike-tripdata.csv, ./2019/201904-citibike-tripdata.csv, ./2019/201907-citibike-tripdata.csv, ./2019/201905-citibike-tripdata.csv, ./2019/201910-citibike-tripdata.csv)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(./2019/201902-citibike-tripdata.csv, ./2019/201911-citibike-tripdata.csv, ./2019/201903-citibike-tripdata.csv, ./2019/201908-citibike-tripdata.csv, ./2019/201912-citibike-tripdata.csv, ./2019/201906-citibike-tripdata.csv, ./2019/201901-citibike-tripdata.csv, ./2019/201909-citibike-tripdata.csv, ./2019/201904-citibike-tripdata.csv, ./2019/201907-citibike-tripdata.csv, ./2019/201905-citibike-tripdata.csv, ./2019/201910-citibike-tripdata.csv)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val folder = \"./2019/\"\n",
    "val csvFiles = getListOfFiles(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desired dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year = 2019\n",
       "month = 02\n",
       "t0 = 2019-02-01 00:00:00.0000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2019-02-01 00:00:00.0000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val year = csvFiles(0).getName().split(\"-\")(0).take(4)\n",
    "val month = csvFiles(0).getName().split(\"-\")(0).takeRight(2)\n",
    "val t0 = s\"${year}-${month}-01 00:00:00.0000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a simple dataset with the following structure:\n",
    "- latitude\n",
    "- longitude\n",
    "- year\n",
    "- month\n",
    "- day\n",
    "- hour\n",
    "- min\n",
    "- (weather)\n",
    "- number of bikes available (target)\n",
    "\n",
    "It will be the input of our learning algorithm. Then, we also have to store some informations about the stations, in another dataset, with the following structure:\n",
    "- station_id\n",
    "- name\n",
    "- latitude\n",
    "- longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single .csv for the stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This .csv consists of four columns:\n",
    "- id\n",
    "- name\n",
    "- latitude\n",
    "- longitude\n",
    "\n",
    "We get all stations from all .csv files within the folder defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newColumns = List(id, name, latitude, longitude)\n",
       "oldStartColumns = List(start station id, start station name, start station latitude, start station longitude)\n",
       "oldEndColumns = List(end station id, end station name, end station latitude, end station longitude)\n",
       "columnsStartList = List(start station id AS `id`, start station name AS `name`, start station latitude AS `latitude`, start station longitude AS `longitude`)\n",
       "columnsEndList = List(end station id AS `id`, end station name AS `name`, end station latitude AS `latitude`, end station longitude AS `longitude`)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(end station id AS `id`, end station name AS `name`, end station latitude AS `latitude`, end station longitude AS `longitude`)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newColumns = Seq(\"id\",\n",
    "                     \"name\",\n",
    "                     \"latitude\",\n",
    "                     \"longitude\"\n",
    "                    )\n",
    "\n",
    "val oldStartColumns = Seq(\"start station id\",\n",
    "                          \"start station name\",\n",
    "                          \"start station latitude\",\n",
    "                          \"start station longitude\"\n",
    "                         )\n",
    "\n",
    "val oldEndColumns = Seq(\"end station id\",\n",
    "                        \"end station name\",\n",
    "                        \"end station latitude\",\n",
    "                        \"end station longitude\"\n",
    "                       )\n",
    "\n",
    "val columnsStartList = oldStartColumns.zip(newColumns).map(f => { col(f._1).as(f._2) })\n",
    "\n",
    "val columnsEndList = oldEndColumns.zip(newColumns).map(f => { col(f._1).as(f._2) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationsDf = [id: string, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var stationsDf = Seq.empty[(String,String,String,String)]\n",
    "                    .toDF(newColumns:_*)\n",
    "\n",
    "for (csvFile <- csvFiles) {\n",
    "    val df = spark.read\n",
    "                  .option(\"header\", \"true\")\n",
    "                  .csv(csvFile.getPath())\n",
    "\n",
    "    val startStations = df.select(df(\"start station id\"),\n",
    "                                  df(\"start station name\"),\n",
    "                                  df(\"start station latitude\"),\n",
    "                                  df(\"start station longitude\")\n",
    "                                 )\n",
    "                           .distinct()\n",
    "                           .select(columnsStartList:_*)\n",
    "    \n",
    "    val endStations = df.select(df(\"end station id\"),\n",
    "                                df(\"end station name\"),\n",
    "                                df(\"end station latitude\"),\n",
    "                                df(\"end station longitude\")\n",
    "                               )\n",
    "                       .distinct()\n",
    "                       .select(columnsEndList:_*)\n",
    "    \n",
    "    stationsDf = stationsDf.union(startStations)\n",
    "                           .union(endStations)\n",
    "                           .distinct()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationsDf = [id: string, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationsDf = stationsDf.dropDuplicates(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationsDf.repartition(1)\n",
    "          .write\n",
    "          .format(\"com.databricks.spark.csv\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .save(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationsDf = [id: string, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Only after running the last cells and renaming the output as \"stations.csv\"\n",
    "\n",
    "val stationsDf = spark.read\n",
    "                      .option(\"header\", \"true\")\n",
    "                      .csv(\"outputs/stations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationsDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the number of bikes available at the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the number of bikes available at the beginning, we just have to select the first start station of each bike and add +1 to the initial number of bikes available in this station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [tripduration: string, starttime: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tripduration: string, starttime: string ... 13 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Let's test with a single .csv\n",
    "\n",
    "val df = spark.read\n",
    "              .option(\"header\", \"true\")\n",
    "              .csv(\"./2019/201901-citibike-tripdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tripduration: string (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      " |-- stoptime: string (nullable = true)\n",
      " |-- start station id: string (nullable = true)\n",
      " |-- start station name: string (nullable = true)\n",
      " |-- start station latitude: string (nullable = true)\n",
      " |-- start station longitude: string (nullable = true)\n",
      " |-- end station id: string (nullable = true)\n",
      " |-- end station name: string (nullable = true)\n",
      " |-- end station latitude: string (nullable = true)\n",
      " |-- end station longitude: string (nullable = true)\n",
      " |-- bikeid: string (nullable = true)\n",
      " |-- usertype: string (nullable = true)\n",
      " |-- birth year: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[time: string, station_id: string ... 1 more field]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "newDf = [time: string, station_id: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val newDf = df.select(df(\"starttime\").as(\"time\"),\n",
    "                      df(\"start station id\").as(\"station_id\"),\n",
    "                      df(\"bikeid\").as(\"bike_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bikeIdStationIdPairs = [station_id: string, one: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[station_id: string, one: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// https://stackoverflow.com/questions/33878370/how-to-select-the-first-row-of-each-group\n",
    "\n",
    "val bikeIdStationIdPairs = newDf.orderBy(\"time\")\n",
    "                                .groupBy(\"bike_id\")\n",
    "                                .agg(first(\"station_id\").alias(\"station_id\"))\n",
    "                                .orderBy(asc(\"station_id\"))\n",
    "                                .filter(\"bike_id != 'NULL'\")\n",
    "                                .groupBy(\"station_id\")\n",
    "                                .agg(count(\"*\").as(\"one\"))\n",
    "                                .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step, with only one input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t0 = 2019-01-01 00:00:00.0000\n",
       "finalDf = [station_id: string, one: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[station_id: string, one: bigint ... 1 more field]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t0 = \"2019-01-01 00:00:00.0000\"\n",
    "var finalDf = bikeIdStationIdPairs.withColumn(\"time\", lit(t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalDf = [station_id: string, time: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[station_id: string, time: string ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = finalDf.select(finalDf(\"station_id\"),\n",
    "                         finalDf(\"time\"),\n",
    "                         finalDf(\"one\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startDf = [station_id: string, time: string ... 1 more field]\n",
       "endDf = [station_id: string, time: string ... 1 more field]\n",
       "dataDf = [station_id: string, time: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[station_id: string, time: string ... 1 more field]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val startDf = df.select(df(\"start station id\").as(\"station_id\"),\n",
    "                       df(\"starttime\").as(\"time\"))\n",
    "               .withColumn(\"one\", lit(-1))\n",
    "\n",
    "val endDf = df.select(df(\"end station id\").as(\"station_id\"),\n",
    "                        df(\"stoptime\").as(\"time\"))\n",
    "                .withColumn(\"one\", lit(1))\n",
    "\n",
    "val dataDf = startDf.union(endDf).sort($\"time\".asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// finalDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// dataDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalDf = [station_id: string, time: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[station_id: string, time: string ... 1 more field]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = finalDf.union(dataDf).sort($\"time\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalDf = [station_id: string, time: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[station_id: string, time: string ... 2 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = finalDf.withColumn(\"nb_bikes_available\", sum(\"one\").over(\n",
    "    Window.partitionBy(\"station_id\")\n",
    "          .orderBy(col(\"time\").asc)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// finalDf.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalDf = [station_id: string, time: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[station_id: string, time: string ... 1 more field]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = finalDf.select(finalDf(\"station_id\"),\n",
    "                         finalDf(\"time\"),\n",
    "                         finalDf(\"nb_bikes_available\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// finalDf.show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationsDf = [id: string, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stationsDf = spark.read\n",
    "                      .option(\"header\", \"true\")\n",
    "                      .csv(\"outputs/stations.csv\")\n",
    "                      .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "Syntax Error.",
     "evalue": "",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// stationsDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalDf = [station_id: string, time: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[station_id: string, time: string ... 5 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = finalDf.join(stationsDf,\n",
    "                       finalDf(\"station_id\") ===  stationsDf(\"id\"),\n",
    "                       \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+---+--------------------+-----------+-----------+\n",
      "|station_id|                time|nb_bikes_available| id|                name|   latitude|  longitude|\n",
      "+----------+--------------------+------------------+---+--------------------+-----------+-----------+\n",
      "|       296|2019-01-01 00:00:...|                29|296|Division St & Bowery|40.71413089|-73.9970468|\n",
      "|       296|2019-01-01 01:01:...|                28|296|Division St & Bowery|40.71413089|-73.9970468|\n",
      "|       296|2019-01-01 01:15:...|                29|296|Division St & Bowery|40.71413089|-73.9970468|\n",
      "|       296|2019-01-01 02:25:...|                30|296|Division St & Bowery|40.71413089|-73.9970468|\n",
      "|       296|2019-01-01 03:26:...|                31|296|Division St & Bowery|40.71413089|-73.9970468|\n",
      "|       296|2019-01-01 06:13:...|                32|296|Division St & Bowery|40.71413089|-73.9970468|\n",
      "|       296|2019-01-01 06:30:...|                31|296|Division St & Bowery|40.71413089|-73.9970468|\n",
      "|       296|2019-01-01 07:44:...|                32|296|Division St & Bowery|40.71413089|-73.9970468|\n",
      "|       296|2019-01-01 08:11:...|                31|296|Division St & Bowery|40.71413089|-73.9970468|\n",
      "|       296|2019-01-01 08:16:...|                30|296|Division St & Bowery|40.71413089|-73.9970468|\n",
      "+----------+--------------------+------------------+---+--------------------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalDf = [latitude: string, longitude: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[latitude: string, longitude: string ... 3 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = finalDf.withColumn(\"_tmp\", split($\"time\", \"\\\\ \"))\n",
    "                 .select(finalDf(\"latitude\"),\n",
    "                         finalDf(\"longitude\"),\n",
    "                         $\"_tmp\".getItem(0).as(\"date\"),\n",
    "                         $\"_tmp\".getItem(1).as(\"time\"),\n",
    "                         finalDf(\"nb_bikes_available\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+-------------+------------------+\n",
      "|   latitude|  longitude|      date|         time|nb_bikes_available|\n",
      "+-----------+-----------+----------+-------------+------------------+\n",
      "|40.71413089|-73.9970468|2019-01-01|00:00:00.0000|                29|\n",
      "|40.71413089|-73.9970468|2019-01-01|01:01:13.0820|                28|\n",
      "|40.71413089|-73.9970468|2019-01-01|01:15:00.9000|                29|\n",
      "|40.71413089|-73.9970468|2019-01-01|02:25:48.7100|                30|\n",
      "|40.71413089|-73.9970468|2019-01-01|03:26:20.0670|                31|\n",
      "|40.71413089|-73.9970468|2019-01-01|06:13:17.5790|                32|\n",
      "|40.71413089|-73.9970468|2019-01-01|06:30:54.8570|                31|\n",
      "|40.71413089|-73.9970468|2019-01-01|07:44:13.5660|                32|\n",
      "|40.71413089|-73.9970468|2019-01-01|08:11:59.0560|                31|\n",
      "|40.71413089|-73.9970468|2019-01-01|08:16:00.5430|                30|\n",
      "+-----------+-----------+----------+-------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finalDf = [latitude: string, longitude: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[latitude: string, longitude: string ... 6 more fields]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = finalDf.withColumn(\"_tmp\", split($\"date\", \"\\\\-\"))\n",
    "                 .withColumn(\"_tmp2\", split($\"time\", \"\\\\:\"))\n",
    "                 .select(finalDf(\"latitude\"),\n",
    "                         finalDf(\"longitude\"),\n",
    "                         $\"_tmp\".getItem(0).as(\"year\"),\n",
    "                         $\"_tmp\".getItem(1).as(\"month\"),\n",
    "                         $\"_tmp\".getItem(2).as(\"day\"),\n",
    "                         $\"_tmp2\".getItem(0).as(\"hour\"),\n",
    "                         $\"_tmp2\".getItem(1).as(\"min\"),\n",
    "                         finalDf(\"nb_bikes_available\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+-----+---+----+---+------------------+\n",
      "|   latitude|  longitude|year|month|day|hour|min|nb_bikes_available|\n",
      "+-----------+-----------+----+-----+---+----+---+------------------+\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  00| 00|                29|\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  01| 01|                28|\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  01| 15|                29|\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  02| 25|                30|\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  03| 26|                31|\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  06| 13|                32|\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  06| 30|                31|\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  07| 44|                32|\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  08| 11|                31|\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  08| 16|                30|\n",
      "+-----------+-----------+----+-----+---+----+---+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf.repartition(1)\n",
    "       .write\n",
    "       .format(\"com.databricks.spark.csv\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .save(\"outputs/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationsDf = [id: string, name: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, name: string ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stationsDf = spark.read\n",
    "                      .option(\"header\", \"true\")\n",
    "                      .csv(\"outputs/stations.csv\")\n",
    "\n",
    "for (csvFile <- csvFiles) {\n",
    "    \n",
    "    val year = csvFile.getName().split(\"-\")(0).take(4)\n",
    "    val month = csvFile.getName().split(\"-\")(0).takeRight(2)\n",
    "    val t0 = s\"${year}-${month}-01 00:00:00.0000\"\n",
    "    \n",
    "    // First: we get initial nb of bikes\n",
    "    \n",
    "    val df = spark.read\n",
    "                  .option(\"header\", \"true\")\n",
    "                  .csv(csvFile.getPath())\n",
    "    \n",
    "    val newDf = df.select(df(\"starttime\").as(\"time\"),\n",
    "                          df(\"start station id\").as(\"station_id\"),\n",
    "                          df(\"bikeid\").as(\"bike_id\"))\n",
    "\n",
    "    val bikeIdStationIdPairs = newDf.orderBy(\"time\")\n",
    "                                    .groupBy(\"bike_id\")\n",
    "                                    .agg(first(\"station_id\").alias(\"station_id\"))\n",
    "                                    .orderBy(asc(\"station_id\"))\n",
    "                                    .filter(\"bike_id != 'NULL'\")\n",
    "                                    .groupBy(\"station_id\")\n",
    "                                    .agg(count(\"*\").as(\"one\"))\n",
    "                                    .cache()\n",
    "    \n",
    "    // Then: we pre-process and join\n",
    "    \n",
    "    var finalDf = bikeIdStationIdPairs.withColumn(\"time\", lit(t0))\n",
    "    \n",
    "    finalDf = finalDf.select(finalDf(\"station_id\"),\n",
    "                             finalDf(\"time\"),\n",
    "                             finalDf(\"one\"))\n",
    "    \n",
    "    val startDf = df.select(df(\"start station id\").as(\"station_id\"),\n",
    "                            df(\"starttime\").as(\"time\"))\n",
    "                    .withColumn(\"one\", lit(-1))\n",
    "\n",
    "    val endDf = df.select(df(\"end station id\").as(\"station_id\"),\n",
    "                          df(\"stoptime\").as(\"time\"))\n",
    "                  .withColumn(\"one\", lit(1))\n",
    "\n",
    "    val dataDf = startDf.union(endDf).sort($\"time\".asc).cache()\n",
    "    \n",
    "    finalDf = finalDf.union(dataDf).sort($\"time\")\n",
    "    \n",
    "    finalDf = finalDf.withColumn(\"nb_bikes_available\",\n",
    "                                 sum(\"one\").over(Window.partitionBy(\"station_id\")\n",
    "                                           .orderBy(col(\"time\").asc))\n",
    "                                )\n",
    "    \n",
    "    finalDf = finalDf.select(finalDf(\"station_id\"),\n",
    "                         finalDf(\"time\"),\n",
    "                         finalDf(\"nb_bikes_available\"))\n",
    "    \n",
    "    finalDf = finalDf.join(stationsDf,\n",
    "                       finalDf(\"station_id\") ===  stationsDf(\"id\"),\n",
    "                       \"inner\")\n",
    "\n",
    "    finalDf = finalDf.withColumn(\"_tmp\", split($\"time\", \"\\\\ \"))\n",
    "                     .select(finalDf(\"latitude\"),\n",
    "                             finalDf(\"longitude\"),\n",
    "                             $\"_tmp\".getItem(0).as(\"date\"),\n",
    "                             $\"_tmp\".getItem(1).as(\"time\"),\n",
    "                             finalDf(\"nb_bikes_available\"))\n",
    "    \n",
    "    finalDf = finalDf.withColumn(\"_tmp\", split($\"date\", \"\\\\-\"))\n",
    "                     .withColumn(\"_tmp2\", split($\"time\", \"\\\\:\"))\n",
    "                     .select(finalDf(\"latitude\"),\n",
    "                             finalDf(\"longitude\"),\n",
    "                             $\"_tmp\".getItem(0).as(\"year\"),\n",
    "                             $\"_tmp\".getItem(1).as(\"month\"),\n",
    "                             $\"_tmp\".getItem(2).as(\"day\"),\n",
    "                             $\"_tmp2\".getItem(0).as(\"hour\"),\n",
    "                             $\"_tmp2\".getItem(1).as(\"min\"),\n",
    "                             finalDf(\"nb_bikes_available\"))\n",
    "    \n",
    "    finalDf.repartition(1)\n",
    "              .write\n",
    "              .format(\"com.databricks.spark.csv\")\n",
    "              .option(\"header\", \"true\")\n",
    "              .save(s\"outputs/${csvFile.getName()}\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with geospatial representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://heartbeat.fritz.ai/working-with-geospatial-data-in-machine-learning-ad4097c7228d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder = ./outputs/dataset/Lat_Long\n",
       "csvFiles = List(./outputs/dataset/Lat_Long/2019-09.csv, ./outputs/dataset/Lat_Long/2019-08.csv, ./outputs/dataset/Lat_Long/2019-04.csv, ./outputs/dataset/Lat_Long/2019-07.csv, ./outputs/dataset/Lat_Long/2019-11.csv, ./outputs/dataset/Lat_Long/2019-03.csv, ./outputs/dataset/Lat_Long/2019-06.csv, ./outputs/dataset/Lat_Long/2019-05.csv, ./outputs/dataset/Lat_Long/2019-12.csv, ./outputs/dataset/Lat_Long/2019-10.csv, ./outputs/dataset/Lat_Long/2019-01.csv, ./outputs/dataset/Lat_Long/2019-02.csv)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(./outputs/dataset/Lat_Long/2019-09.csv, ./outputs/dataset/Lat_Long/2019-08.csv, ./outputs/dataset/Lat_Long/2019-04.csv, ./outputs/dataset/Lat_Long/2019-07.csv, ./outputs/dataset/Lat_Long/2019-11.csv, ./outputs/dataset/Lat_Long/2019-03.csv, ./outputs/dataset/Lat_Long/2019-06.csv, ./outputs/dataset/Lat_Long/2019-05.csv, ./outputs/dataset/Lat_Long/2019-12.csv, ./outputs/dataset/Lat_Long/2019-10.csv, ./outputs/dataset/Lat_Long/2019-01.csv, ./outputs/dataset/Lat_Long/2019-02.csv)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val folder = \"./outputs/dataset/Lat_Long\"\n",
    "val csvFiles = getListOfFiles(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computeXCooUdf = UserDefinedFunction(<function2>,DoubleType,Some(List(LongType, LongType)))\n",
       "computeYCooUdf = UserDefinedFunction(<function2>,DoubleType,Some(List(LongType, LongType)))\n",
       "computeZCooUdf = UserDefinedFunction(<function1>,DoubleType,Some(List(LongType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n",
       "computeXCoo: (latitude: Long, longitude: Long)Double\n",
       "computeYCoo: (latitude: Long, longitude: Long)Double\n",
       "computeZCoo: (latitude: Long)Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,DoubleType,Some(List(LongType)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeXCoo(latitude: Long, longitude: Long): Double = {\n",
    "    return cos(latitude) * cos(longitude)\n",
    "}\n",
    "\n",
    "def computeYCoo(latitude: Long, longitude: Long): Double = {\n",
    "    return cos(latitude) * sin(longitude)\n",
    "}\n",
    "\n",
    "def computeZCoo(latitude: Long): Double = {\n",
    "    return sin(latitude)\n",
    "}\n",
    "\n",
    "val computeXCooUdf = udf(computeXCoo _)\n",
    "val computeYCooUdf = udf(computeYCoo _)\n",
    "val computeZCooUdf = udf(computeZCoo _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [latitude: string, longitude: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[latitude: string, longitude: string ... 6 more fields]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read\n",
    "              .option(\"header\", \"true\")\n",
    "              .csv(csvFiles(0).getPath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- min: string (nullable = true)\n",
      " |-- nb_bikes_available: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+-----+---+----+---+------------------+\n",
      "|   latitude|  longitude|year|month|day|hour|min|nb_bikes_available|\n",
      "+-----------+-----------+----+-----+---+----+---+------------------+\n",
      "|40.71413089|-73.9970468|2019|   01| 01|  00| 00|                31|\n",
      "|40.71413089|-73.9970468|2019|   09| 01|  00| 41|                30|\n",
      "|40.71413089|-73.9970468|2019|   09| 01|  00| 59|                31|\n",
      "|40.71413089|-73.9970468|2019|   09| 01|  01| 05|                30|\n",
      "|40.71413089|-73.9970468|2019|   09| 01|  01| 15|                31|\n",
      "|40.71413089|-73.9970468|2019|   09| 01|  02| 13|                30|\n",
      "|40.71413089|-73.9970468|2019|   09| 01|  02| 13|                29|\n",
      "|40.71413089|-73.9970468|2019|   09| 01|  02| 36|                30|\n",
      "|40.71413089|-73.9970468|2019|   09| 01|  02| 48|                29|\n",
      "|40.71413089|-73.9970468|2019|   09| 01|  02| 55|                28|\n",
      "+-----------+-----------+----+-----+---+----+---+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newDf = [x_coo: double, y_coo: double ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[x_coo: double, y_coo: double ... 7 more fields]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// https://stackoverflow.com/questions/35227568/applying-function-to-spark-dataframe-column\n",
    "\n",
    "val newDf = df.withColumn(\"x_coo\", computeXCooUdf($\"latitude\", $\"longitude\"))\n",
    "              .withColumn(\"y_coo\", computeYCooUdf($\"latitude\", $\"longitude\"))\n",
    "              .withColumn(\"z_coo\", computeZCooUdf($\"latitude\"))\n",
    "              .select($\"x_coo\",\n",
    "                      $\"y_coo\",\n",
    "                      $\"z_coo\",\n",
    "                      df(\"year\"),\n",
    "                      df(\"month\"),\n",
    "                      df(\"day\"),\n",
    "                      df(\"hour\"),\n",
    "                      df(\"min\"),\n",
    "                      df(\"nb_bikes_available\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------------------+----+-----+---+----+---+------------------+\n",
      "|             x_coo|              y_coo|             z_coo|year|month|day|hour|min|nb_bikes_available|\n",
      "+------------------+-------------------+------------------+----+-----+---+----+---+------------------+\n",
      "|0.4909949444970359|-0.4513649771070291|0.7451131604793488|2019|   01| 01|  00| 00|                31|\n",
      "|0.4909949444970359|-0.4513649771070291|0.7451131604793488|2019|   09| 01|  00| 41|                30|\n",
      "|0.4909949444970359|-0.4513649771070291|0.7451131604793488|2019|   09| 01|  00| 59|                31|\n",
      "|0.4909949444970359|-0.4513649771070291|0.7451131604793488|2019|   09| 01|  01| 05|                30|\n",
      "|0.4909949444970359|-0.4513649771070291|0.7451131604793488|2019|   09| 01|  01| 15|                31|\n",
      "|0.4909949444970359|-0.4513649771070291|0.7451131604793488|2019|   09| 01|  02| 13|                30|\n",
      "|0.4909949444970359|-0.4513649771070291|0.7451131604793488|2019|   09| 01|  02| 13|                29|\n",
      "|0.4909949444970359|-0.4513649771070291|0.7451131604793488|2019|   09| 01|  02| 36|                30|\n",
      "|0.4909949444970359|-0.4513649771070291|0.7451131604793488|2019|   09| 01|  02| 48|                29|\n",
      "|0.4909949444970359|-0.4513649771070291|0.7451131604793488|2019|   09| 01|  02| 55|                28|\n",
      "+------------------+-------------------+------------------+----+-----+---+----+---+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (csvFile <- csvFiles) {\n",
    "    val df = spark.read\n",
    "                  .option(\"header\", \"true\")\n",
    "                  .csv(csvFile.getPath())\n",
    "\n",
    "    val newDf = df.withColumn(\"x_coo\", computeXCooUdf($\"latitude\", $\"longitude\"))\n",
    "                  .withColumn(\"y_coo\", computeYCooUdf($\"latitude\", $\"longitude\"))\n",
    "                  .withColumn(\"z_coo\", computeZCooUdf($\"latitude\"))\n",
    "                  .select($\"x_coo\",\n",
    "                          $\"y_coo\",\n",
    "                          $\"z_coo\",\n",
    "                          df(\"year\"),\n",
    "                          df(\"month\"),\n",
    "                          df(\"day\"),\n",
    "                          df(\"hour\"),\n",
    "                          df(\"min\"),\n",
    "                          df(\"nb_bikes_available\"))\n",
    "    \n",
    "    newDf.repartition(1)\n",
    "         .write\n",
    "         .format(\"com.databricks.spark.csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .save(s\"outputs/${csvFile.getName()}\")\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
