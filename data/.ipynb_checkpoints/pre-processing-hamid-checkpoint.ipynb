{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import java.io.File\n",
    "import scala.collection.mutable.ListBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@4ce3d48c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@4ce3d48c"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.master(\"local[*]\").appName(\"SparkPreProcessing\").getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getListOfFiles: (dir: String)List[java.io.File]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// http://alvinalexander.com/scala/how-to-list-files-in-directory-filter-names-scala/\n",
    "\n",
    "def getListOfFiles(dir: String):List[File] = {\n",
    "    val d = new File(dir)\n",
    "    if (d.exists && d.isDirectory) {\n",
    "        d.listFiles.filter(_.isFile).toList\n",
    "    } else {\n",
    "        List[File]()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder = ./2019/\n",
       "csvFiles = List()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val folder = \"./2019/\"\n",
    "val csvFiles = getListOfFiles(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desired dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a simple dataset with the following structure:\n",
    "- station_id\n",
    "- time\n",
    "- (weather)\n",
    "- number of bikes available (target)\n",
    "\n",
    "It will be the input of our learning algorithm. Then, we also have to store some informations about the stations, in another dataset, with the following structure:\n",
    "- station_id\n",
    "- name\n",
    "- latitude\n",
    "- longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single .csv for the stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This .csv consists of four columns:\n",
    "- id\n",
    "- name\n",
    "- latitude\n",
    "- longitude\n",
    "\n",
    "We get all stations from all .csv files within the folder defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newColumns = List(id, name, latitude, longitude)\n",
       "oldStartColumns = List(start station id, start station name, start station latitude, start station longitude)\n",
       "oldEndColumns = List(end station id, end station name, end station latitude, end station longitude)\n",
       "columnsStartList = List(start station id AS `id`, start station name AS `name`, start station latitude AS `latitude`, start station longitude AS `longitude`)\n",
       "columnsEndList = List(end station id AS `id`, end station name AS `name`, end station latitude AS `latitude`, end station longitude AS `longitude`)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(end station id AS `id`, end station name AS `name`, end station latitude AS `latitude`, end station longitude AS `longitude`)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newColumns = Seq(\"id\",\n",
    "                     \"name\",\n",
    "                     \"latitude\",\n",
    "                     \"longitude\"\n",
    "                    )\n",
    "\n",
    "val oldStartColumns = Seq(\"start station id\",\n",
    "                          \"start station name\",\n",
    "                          \"start station latitude\",\n",
    "                          \"start station longitude\"\n",
    "                         )\n",
    "\n",
    "val oldEndColumns = Seq(\"end station id\",\n",
    "                        \"end station name\",\n",
    "                        \"end station latitude\",\n",
    "                        \"end station longitude\"\n",
    "                       )\n",
    "\n",
    "val columnsStartList = oldStartColumns.zip(newColumns).map(f => { col(f._1).as(f._2) })\n",
    "\n",
    "val columnsEndList = oldEndColumns.zip(newColumns).map(f => { col(f._1).as(f._2) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:39: error: not found: value newColumns\n       var stationsDf = Seq.empty[(String,String,String,String)].toDF(newColumns:_*)\n                                                                      ^\n<console>:54: error: not found: value columnsStartList\n                                  .select(columnsStartList:_*)\n                                          ^\n<console>:62: error: not found: value columnsEndList\n                              .select(columnsEndList:_*)\n                                      ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "var stationsDf = Seq.empty[(String,String,String,String)].toDF(newColumns:_*)\n",
    "\n",
    "for (csvFile <- csvFiles) {\n",
    "    val df = spark.read\n",
    "                  .option(\"header\", \"true\")\n",
    "                  .csv(csvFile.getPath())\n",
    "\n",
    "    val startStations = df.select(df(\"start station id\"),\n",
    "                                  df(\"start station name\"),\n",
    "                                  df(\"start station latitude\"),\n",
    "                                  df(\"start station longitude\")\n",
    "                                 )\n",
    "                           .distinct()\n",
    "                           .select(columnsStartList:_*)\n",
    "    \n",
    "    val endStations = df.select(df(\"end station id\"),\n",
    "                                df(\"end station name\"),\n",
    "                                df(\"end station latitude\"),\n",
    "                                df(\"end station longitude\")\n",
    "                               )\n",
    "                       .distinct()\n",
    "                       .select(columnsEndList:_*)\n",
    "    \n",
    "    stationsDf = stationsDf.union(startStations).union(endStations).distinct()\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:38: error: not found: value stationsDf\n       stationsDf.repartition(1)\n       ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "stationsDf.repartition(1)\n",
    "          .write\n",
    "          .format(\"com.databricks.spark.csv\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .save(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "Path does not exist: file:/media/sf_codes/id2221_project/data/stations.csv;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Path does not exist: file:/media/sf_codes/id2221_project/data/stations.csv;",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)",
      "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)",
      "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)",
      "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)",
      "  at scala.collection.immutable.List.flatMap(List.scala:355)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)",
      "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:615)",
      "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:467)",
      "  ... 46 elided"
     ]
    }
   ],
   "source": [
    "// Only after running the last cells and renaming the output as \"stations.csv\"\n",
    "\n",
    "val stationsDf = spark.read\n",
    "                   .option(\"header\", \"true\")\n",
    "                   .csv(\"stations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "lastException: Throwable = null\n<console>:38: error: not found: value stationsDf\n       stationsDf.count()\n       ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "stationsDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the number of bikes available at the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the number of bikes available at the beginning, we just have to select the first start station of each bike and add +1 to the initial number of bikes available in this station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [tripduration: string, starttime: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tripduration: string, starttime: string ... 13 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Let's test with a single .csv\n",
    "\n",
    "val df = spark.read\n",
    "              .option(\"header\", \"true\")\n",
    "              .csv(\"./raw_data/2019/201901-citibike-tripdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tripduration: string (nullable = true)\n",
      " |-- starttime: string (nullable = true)\n",
      " |-- stoptime: string (nullable = true)\n",
      " |-- start station id: string (nullable = true)\n",
      " |-- start station name: string (nullable = true)\n",
      " |-- start station latitude: string (nullable = true)\n",
      " |-- start station longitude: string (nullable = true)\n",
      " |-- end station id: string (nullable = true)\n",
      " |-- end station name: string (nullable = true)\n",
      " |-- end station latitude: string (nullable = true)\n",
      " |-- end station longitude: string (nullable = true)\n",
      " |-- bikeid: string (nullable = true)\n",
      " |-- usertype: string (nullable = true)\n",
      " |-- birth year: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newDf = [time: string, station_id: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[time: string, station_id: string ... 1 more field]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newDf = df.select(df(\"starttime\").as(\"time\"),\n",
    "                      df(\"start station id\").as(\"station_id\"),\n",
    "                      df(\"bikeid\").as(\"bike_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bikeIdStationIdPairs = MapPartitionsRDD[118] at rdd at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[118] at rdd at <console>:41"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// https://stackoverflow.com/questions/33878370/how-to-select-the-first-row-of-each-group\n",
    "\n",
    "val bikeIdStationIdPairs = newDf.orderBy(\"time\")\n",
    "                                .groupBy(\"bike_id\")\n",
    "                                .agg(first(\"station_id\").alias(\"station_id\"))\n",
    "                                .orderBy(asc(\"station_id\"))\n",
    "                                .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stationIdInitialNbOfBikesPairs = ShuffledRDD[121] at reduceByKey at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[121] at reduceByKey at <console>:41"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stationIdInitialNbOfBikesPairs = bikeIdStationIdPairs.filter(_(1) != \"NULL\")\n",
    "                                                         .map(x => (x(1), 1))\n",
    "                                                         .reduceByKey(_ + _)\n",
    "//                                                          .sortBy(_._1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
